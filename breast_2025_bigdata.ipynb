{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHTbKPWELIB7hmOCNIrypt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahnavivummaneni/Breast-Cancer-EarlyPrediction/blob/main/breast_2025_bigdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ww8V_1gcnuA",
        "outputId": "8555ad78-9f94-4c76-da14-93597013c386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-18 12:55:08--  https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299350810 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.1-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.3.1-bin-had 100%[===================>] 285.48M  17.5MB/s    in 17s     \n",
            "\n",
            "2025-07-18 12:55:26 (16.5 MB/s) - ‘spark-3.3.1-bin-hadoop3.tgz’ saved [299350810/299350810]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# 2. Download Spark 3.3.1 (from Apache archive)\n",
        "!wget -O spark-3.3.1-bin-hadoop3.tgz https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "\n",
        "# 3. Extract Spark\n",
        "!tar -xzf spark-3.3.1-bin-hadoop3.tgz\n",
        "\n",
        "# 4. Install findspark\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "t-_DecHScoxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Initialize SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BreastCancerPrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 3: Load the CSV dataset\n",
        "df = spark.read.csv(\"breast-cancer-dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 4: Print schema\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIQp0qQ4dhFB",
        "outputId": "e389ac4a-c3ea-4216-d134-132aeb868f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- S/N: integer (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Menopause: integer (nullable = true)\n",
            " |-- Tumor Size (cm): string (nullable = true)\n",
            " |-- Inv-Nodes: string (nullable = true)\n",
            " |-- Breast: string (nullable = true)\n",
            " |-- Metastasis: string (nullable = true)\n",
            " |-- Breast Quadrant: string (nullable = true)\n",
            " |-- History: string (nullable = true)\n",
            " |-- Diagnosis Result: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 1: Cast numeric columns to double\n",
        "df = df.withColumn(\"Age\", col(\"Age\").cast(\"double\")) \\\n",
        "       .withColumn(\"Tumor Size (cm)\", col(\"Tumor Size (cm)\").cast(\"double\")) \\\n",
        "       .withColumn(\"Inv-Nodes\", col(\"Inv-Nodes\").cast(\"double\")) \\\n",
        "       .withColumn(\"Metastasis\", col(\"Metastasis\").cast(\"double\"))\n",
        "\n",
        "# Step 2: Index categorical columns\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"Diagnosis Result\", outputCol=\"Diagnosis_idx\"),\n",
        "    StringIndexer(inputCol=\"Menopause\", outputCol=\"Menopause_idx\"),\n",
        "    StringIndexer(inputCol=\"Breast\", outputCol=\"Breast_idx\"),\n",
        "    StringIndexer(inputCol=\"Breast Quadrant\", outputCol=\"Quadrant_idx\")\n",
        "]\n",
        "\n",
        "# Step 3: Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Age\", \"Tumor Size (cm)\", \"Inv-Nodes\", \"Metastasis\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Step 4: Create pipeline and transform\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "processed_df = pipeline.fit(df).transform(df)\n",
        "\n",
        "# Step 5: Show result\n",
        "processed_df.select(\"features\", \"Diagnosis_idx\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp8jUWnAdrCQ",
        "outputId": "6c552ed3-ae3e-439a-a142-64dca2f144ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------+\n",
            "|          features|Diagnosis_idx|\n",
            "+------------------+-------------+\n",
            "|[40.0,2.0,0.0,0.0]|          0.0|\n",
            "|[39.0,2.0,0.0,0.0]|          0.0|\n",
            "|[45.0,4.0,0.0,0.0]|          0.0|\n",
            "|[26.0,3.0,0.0,0.0]|          0.0|\n",
            "|[21.0,1.0,0.0,0.0]|          0.0|\n",
            "+------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any rows with nulls in relevant columns\n",
        "df_clean = df.dropna(subset=[\"Age\", \"Tumor Size (cm)\", \"Inv-Nodes\", \"Metastasis\"])\n",
        "\n",
        "# Reapply pipeline\n",
        "processed_df = pipeline.fit(df_clean).transform(df_clean)\n"
      ],
      "metadata": {
        "id": "SFcsJxkRe5st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "# Impute missing values with mean\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\", \"Tumor Size (cm)\", \"Inv-Nodes\", \"Metastasis\"],\n",
        "    outputCols=[\"Age\", \"Tumor Size (cm)\", \"Inv-Nodes\", \"Metastasis\"]\n",
        ")\n",
        "\n",
        "df_imputed = imputer.fit(df).transform(df)\n",
        "processed_df = pipeline.fit(df_imputed).transform(df_imputed)\n"
      ],
      "metadata": {
        "id": "njR5AzYYgy4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Age\", \"Tumor Size (cm)\", \"Inv-Nodes\", \"Metastasis\"],\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"  # <-- NEW!\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "processed_df = pipeline.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "L70f8mmniEga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the cleaned data\n",
        "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training samples: {train_df.count()}\")\n",
        "print(f\"Testing samples: {test_df.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZzxCa7wjufM",
        "outputId": "a7e574ce-5dcf-4c06-a08e-be54d963c87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 179\n",
            "Testing samples: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(\n",
        "    labelCol=\"Diagnosis_idx\",\n",
        "    featuresCol=\"features\",\n",
        "    maxIter=100,\n",
        "    maxDepth=5,\n",
        "    stepSize=0.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)\n"
      ],
      "metadata": {
        "id": "bnOYh_oGkHiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Diagnosis_idx\", predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "for metric in [\"accuracy\", \"f1\", \"weightedPrecision\", \"weightedRecall\"]:\n",
        "    evaluator.setMetricName(metric)\n",
        "    score = evaluator.evaluate(gbt_predictions)\n",
        "    print(f\"{metric.replace('weighted', 'Weighted ').title()}: {score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esfMYGyTlNoP",
        "outputId": "50aad280-1f24-43f7-eaf7-356a44e7c1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.91\n",
            "F1: 0.91\n",
            "Weighted Precision: 0.92\n",
            "Weighted Recall: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9OTaUUrln_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}